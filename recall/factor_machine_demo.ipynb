{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TensorFlow2.Xで構築\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC\n",
    "import os\n",
    "\n",
    "# All the data could be downloaded from: https://www.kaggle.com/datasets/mrkmakr/criteo-dataset\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncriteo dataset preprocessing\\n\\ncriteo data feature introduce：\\n- Label - Y label，「Click」value=1，「Non-Click」value=0\\n- I1-I13 - Totally 13 col integer data feature（most of them are counting features）\\n- C1-C26 - Totally 26 col catagory features，for secure reason，original data was transformed to 32 bit data\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "criteo dataset preprocessing\n",
    "\n",
    "criteo data feature introduce：\n",
    "- Label - Y label，「Click」value=1，「Non-Click」value=0\n",
    "- I1-I13 - Totally 13 col integer data feature（most of them are counting features）\n",
    "- C1-C26 - Totally 26 col catagory features，for secure reason，original data was transformed to 32 bit data\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def sparse_feature(feat, feat_num, embed_dim=4):\n",
    "    \"\"\"\n",
    "    Build a dictionary for sparse features\n",
    "    :@param feat: features name\n",
    "    :@param feat_num: The number of sparse features that are not repeated\n",
    "    :@param embed_dim: Dimension of the feature embedding\n",
    "    \"\"\"\n",
    "    return {'feat_name': feat, 'feat_num': feat_num, 'embed_dim': embed_dim}\n",
    "\n",
    "\n",
    "def dense_feature(feat):\n",
    "    \"\"\"\n",
    "    Build dictionaries for dense (numerical) type features\n",
    "    :@param feat: features name\n",
    "    \"\"\"\n",
    "    return {'feat_name': feat}\n",
    "\n",
    "\n",
    "def create_criteo_dataset(file, embed_dim=8, read_part=True, sample_num=100000, test_size=0.2):\n",
    "    \"\"\"\n",
    "    criteo data set preprocessing\n",
    "    :@param file: data file path\n",
    "    :@param embed_dim: The embedding dimension of sparse features\n",
    "    :@param read_part: Read partial data (best set to True if the data set is large)\n",
    "    :@param sample_num: Sample size for each part under the partial read form\n",
    "    :@param test_size: Test set ratio\n",
    "    \"\"\"\n",
    "    names = ['label', 'I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10', 'I11', 'I12', 'I13',\n",
    "             'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n",
    "             'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'C22',\n",
    "             'C23', 'C24', 'C25', 'C26']\n",
    "\n",
    "    # Partial read and full read\n",
    "    if read_part:\n",
    "        data_df = pd.read_csv(file, sep='\\t', iterator=True, header=None, names=names)\n",
    "        data_df = data_df.get_chunk(sample_num)\n",
    "\n",
    "    else:\n",
    "        data_df = pd.read_csv(file, sep='\\t', header=None, names=names)\n",
    "\n",
    "    # Specify sparse and dense features\n",
    "    sparse_features = ['C' + str(i) for i in range(1, 27)]\n",
    "    dense_features = ['I' + str(i) for i in range(1, 14)]\n",
    "    features = sparse_features + dense_features\n",
    "\n",
    "    # Missing value filling\n",
    "    data_df[sparse_features] = data_df[sparse_features].fillna('nan')\n",
    "    data_df[dense_features] = data_df[dense_features].fillna(0)\n",
    "\n",
    "    # discretization\n",
    "    est = KBinsDiscretizer(n_bins=100, encode='ordinal', strategy='uniform')\n",
    "    data_df[dense_features] = est.fit_transform(data_df[dense_features])\n",
    "\n",
    "    for feat in sparse_features:\n",
    "        le = LabelEncoder()\n",
    "        data_df[feat] = le.fit_transform(data_df[feat])\n",
    "\n",
    "    # Feature engineering: embedding of discrete features\n",
    "    feature_columns = [sparse_feature(feat, int(data_df[feat].max()) + 1, embed_dim=embed_dim) for feat in features]\n",
    "    train, test = train_test_split(data_df, test_size=test_size)\n",
    "\n",
    "    # Generate training and test sets\n",
    "    train_X = train[features].values.astype('int32')\n",
    "    train_y = train['label'].values.astype('int32')\n",
    "    test_X = test[features].values.astype('int32')\n",
    "    test_y = test['label'].values.astype('int32')\n",
    "\n",
    "    return feature_columns, (train_X, train_y), (test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(Layer):\n",
    "    def __init__(self, feature_columns, k, w_r=1e-6, v_r=1e-6):\n",
    "        \"\"\"\n",
    "        FM Model\n",
    "        :@param feature_columns: A list. sparse column feature information.\n",
    "        :@param k: Implicit vector dimension\n",
    "        :@param w_r: The regularization coefficient of the parameter w\n",
    "        :@param v_r: The regularization coefficient of the parameter v\n",
    "        \"\"\"\n",
    "        super(MyLayer, self).__init__()\n",
    "        self.sparse_feature_columns = feature_columns\n",
    "        self.index_mapping = []\n",
    "        self.feature_length = 0\n",
    "        for feat in self.sparse_feature_columns:\n",
    "            self.index_mapping.append(self.feature_length)\n",
    "            self.feature_length += feat['feat_num']\n",
    "        self.k = k\n",
    "        self.w_r = w_r\n",
    "        self.v_r = v_r\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w0 = self.add_weight(name='w0', shape=(1,),\n",
    "                                  initializer=tf.zeros_initializer(),\n",
    "                                  trainable=True)\n",
    "        self.w = self.add_weight(name='w', shape=(self.feature_length, 1),\n",
    "                                 initializer=tf.random_normal_initializer(),\n",
    "                                 regularizer=l2(self.w_r),\n",
    "                                 trainable=True)\n",
    "        self.V = self.add_weight(name='V', shape=(self.feature_length, self.k),\n",
    "                                 initializer=tf.random_normal_initializer(),\n",
    "                                 regularizer=l2(self.v_r),\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # mapping\n",
    "        inputs = inputs + tf.convert_to_tensor(self.index_mapping)\n",
    "        \n",
    "        # First-order term\n",
    "        first_order = self.w0 + tf.reduce_sum(tf.nn.embedding_lookup(self.w, inputs), axis=1)  # (batch_size, 1)\n",
    "        \n",
    "        # second-order term\n",
    "        second_inputs = tf.nn.embedding_lookup(self.V, inputs)  # (batch_size, fields, embed_dim)\n",
    "        square_sum = tf.square(tf.reduce_sum(second_inputs, axis=1, keepdims=True))  # (batch_size, 1, embed_dim)\n",
    "        sum_square = tf.reduce_sum(tf.square(second_inputs), axis=1, keepdims=True)  # (batch_size, 1, embed_dim)\n",
    "        second_order = 0.5 * tf.reduce_sum(square_sum - sum_square, axis=2)  # (batch_size, 1)\n",
    "        \n",
    "        # First-order+second-order\n",
    "        outputs = first_order + second_order\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(Model):\n",
    "    def __init__(self, feature_columns, k, w_r=1e-6, v_r=1e-6):\n",
    "        \"\"\"\n",
    "        Factorization Machines\n",
    "        :param feature_columns: A list. sparse column feature information.\n",
    "        :param k: the latent vector\n",
    "        :param w_r: the regularization coefficient of parameter w\n",
    "\t\t:param v_r: the regularization coefficient of parameter v\n",
    "        \"\"\"\n",
    "        super(FM, self).__init__()\n",
    "        self.sparse_feature_columns = feature_columns\n",
    "        self.fm = MyLayer(feature_columns, k, w_r, v_r)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        fm_outputs = self.fm(inputs)\n",
    "        outputs = tf.nn.sigmoid(fm_outputs)\n",
    "        return outputs\n",
    "\n",
    "    def summary(self, **kwargs):\n",
    "        sparse_inputs = Input(shape=(len(self.sparse_feature_columns),), dtype=tf.int32)\n",
    "        Model(inputs=sparse_inputs, outputs=self.call(sparse_inputs)).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setting\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "#フォルダ\n",
    "dataDir = '/Users/hayden/Documents/data'#フォルダ\n",
    "\n",
    "# Hyperparameter setting\n",
    "file = dataDir + '/Criteo_dataset/train.txt'\n",
    "read_part = True\n",
    "sample_num = 200000\n",
    "test_size = 0.2\n",
    "\n",
    "k = 8\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 512\n",
    "epochs = 10\n",
    "\n",
    "# Build data set\n",
    "feature_columns, train, test = create_criteo_dataset(file=file,\n",
    "                                        read_part=read_part,\n",
    "                                        sample_num=sample_num,\n",
    "                                        test_size=test_size)\n",
    "train_X, train_y = train\n",
    "test_X, test_y = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 39)]              0         \n",
      "                                                                 \n",
      " my_layer (MyLayer)          (None, 1)                 3751795   \n",
      "                                                                 \n",
      " tf.math.sigmoid (TFOpLambd  (None, 1)                 0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3751795 (14.31 MB)\n",
      "Trainable params: 3751795 (14.31 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 21:49:21.413246: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/266 [==============================] - ETA: 0s - loss: 0.5057 - auc: 0.7128"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 21:49:28.052561: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/266 [==============================] - 7s 19ms/step - loss: 0.5057 - auc: 0.7128 - val_loss: 0.4688 - val_auc: 0.7635\n",
      "Epoch 2/10\n",
      "266/266 [==============================] - 5s 18ms/step - loss: 0.4063 - auc: 0.8444 - val_loss: 0.4698 - val_auc: 0.7666\n",
      "Epoch 3/10\n",
      "266/266 [==============================] - 5s 19ms/step - loss: 0.3262 - auc: 0.9088 - val_loss: 0.4925 - val_auc: 0.7541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a82a9900>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Model building\n",
    "model = FM(feature_columns=feature_columns, k=k)\n",
    "model.summary()\n",
    "model.compile(loss=binary_crossentropy, \n",
    "                optimizer=Adam(learning_rate=learning_rate),\n",
    "                metrics=[AUC()])\n",
    "\n",
    "# Model training\n",
    "model.fit(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    epochs=epochs,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)],  # checkpoint\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.15\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 1s 11ms/step - loss: 0.4743 - auc: 0.7607\n",
      "test AUC: 0.760714\n"
     ]
    }
   ],
   "source": [
    "# Performance on testing data set\n",
    "print('test AUC: %f' % model.evaluate(test_X, test_y, batch_size=batch_size)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   12,   118, 83059, ...,     1,     0,     0],\n",
       "       [  148,    62, 62800, ...,    12,     0,     0],\n",
       "       [   12,   296, 10578, ...,     6,     0,     0],\n",
       "       ...,\n",
       "       [  301,   118, 68646, ...,     2,     0,     0],\n",
       "       [   12,   443, 56302, ...,     0,     0,     0],\n",
       "       [   12,   487, 35936, ...,     3,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[   38,   118, 83059, ...,     2,     0,     0],\n",
       "       [   12,   223, 67549, ...,     0,     0,     0],\n",
       "       [  522,    71,   904, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [   12,   136, 78876, ...,     0,     0,     0],\n",
       "       [   12,   487,  8716, ...,     1,     0,     0],\n",
       "       [   12,   134, 66276, ...,     3,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160000, 39)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(40000, 39)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape\n",
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'my_layer/V:0' shape=(416866, 8) dtype=float32, numpy=\n",
       "array([[-0.07061051,  0.03911226,  0.00684353, ..., -0.02671424,\n",
       "         0.0013881 ,  0.08055133],\n",
       "       [ 0.01250868, -0.01755876,  0.01218676, ..., -0.02601379,\n",
       "         0.01278361, -0.03148063],\n",
       "       [ 0.04311165,  0.01400692,  0.00356985, ...,  0.00036511,\n",
       "         0.00878077,  0.04960729],\n",
       "       ...,\n",
       "       [ 0.07237215,  0.0586164 , -0.050928  , ...,  0.00987792,\n",
       "        -0.04890952, -0.01900741],\n",
       "       [ 0.00903635, -0.00232638,  0.00441977, ..., -0.00183274,\n",
       "        -0.00590627, -0.00448157],\n",
       "       [-0.03557094,  0.04855461, -0.03319177, ...,  0.07543939,\n",
       "         0.01555497, -0.00623946]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fm.V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'my_layer/w:0' shape=(416866, 1) dtype=float32, numpy=\n",
       "array([[ 0.06552668],\n",
       "       [-0.04119697],\n",
       "       [ 0.05113696],\n",
       "       ...,\n",
       "       [ 0.00861098],\n",
       "       [-0.02402387],\n",
       "       [-0.03423011]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fm.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'my_layer/w0:0' shape=(1,) dtype=float32, numpy=array([-0.05739688], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fm.w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListWrapper([0, 733, 1247, 84307, 127606, 127786, 127799, 136560, 136914, 136917, 152232, 156355, 233179, 236094, 236120, 242537, 305675, 305685, 308688, 310212, 310216, 381397, 381411, 381426, 400703, 400758, 415566, 415666, 415766, 415866, 415966, 416066, 416166, 416266, 416366, 416466, 416566, 416666, 416766])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fm.index_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListWrapper([DictWrapper({'feat_name': 'C1', 'feat_num': 733, 'embed_dim': 8}), DictWrapper({'feat_name': 'C2', 'feat_num': 514, 'embed_dim': 8}), DictWrapper({'feat_name': 'C3', 'feat_num': 83060, 'embed_dim': 8}), DictWrapper({'feat_name': 'C4', 'feat_num': 43299, 'embed_dim': 8}), DictWrapper({'feat_name': 'C5', 'feat_num': 180, 'embed_dim': 8}), DictWrapper({'feat_name': 'C6', 'feat_num': 13, 'embed_dim': 8}), DictWrapper({'feat_name': 'C7', 'feat_num': 8761, 'embed_dim': 8}), DictWrapper({'feat_name': 'C8', 'feat_num': 354, 'embed_dim': 8}), DictWrapper({'feat_name': 'C9', 'feat_num': 3, 'embed_dim': 8}), DictWrapper({'feat_name': 'C10', 'feat_num': 15315, 'embed_dim': 8}), DictWrapper({'feat_name': 'C11', 'feat_num': 4123, 'embed_dim': 8}), DictWrapper({'feat_name': 'C12', 'feat_num': 76824, 'embed_dim': 8}), DictWrapper({'feat_name': 'C13', 'feat_num': 2915, 'embed_dim': 8}), DictWrapper({'feat_name': 'C14', 'feat_num': 26, 'embed_dim': 8}), DictWrapper({'feat_name': 'C15', 'feat_num': 6417, 'embed_dim': 8}), DictWrapper({'feat_name': 'C16', 'feat_num': 63138, 'embed_dim': 8}), DictWrapper({'feat_name': 'C17', 'feat_num': 10, 'embed_dim': 8}), DictWrapper({'feat_name': 'C18', 'feat_num': 3003, 'embed_dim': 8}), DictWrapper({'feat_name': 'C19', 'feat_num': 1524, 'embed_dim': 8}), DictWrapper({'feat_name': 'C20', 'feat_num': 4, 'embed_dim': 8}), DictWrapper({'feat_name': 'C21', 'feat_num': 71181, 'embed_dim': 8}), DictWrapper({'feat_name': 'C22', 'feat_num': 14, 'embed_dim': 8}), DictWrapper({'feat_name': 'C23', 'feat_num': 15, 'embed_dim': 8}), DictWrapper({'feat_name': 'C24', 'feat_num': 19277, 'embed_dim': 8}), DictWrapper({'feat_name': 'C25', 'feat_num': 55, 'embed_dim': 8}), DictWrapper({'feat_name': 'C26', 'feat_num': 14808, 'embed_dim': 8}), DictWrapper({'feat_name': 'I1', 'feat_num': 100, 'embed_dim': 8}), DictWrapper({'feat_name': 'I2', 'feat_num': 100, 'embed_dim': 8}), DictWrapper({'feat_name': 'I3', 'feat_num': 100, 'embed_dim': 8}), DictWrapper({'feat_name': 'I4', 'feat_num': 100, 'embed_dim': 8}), DictWrapper({'feat_name': 'I5', 'feat_num': 100, 'embed_dim': 8}), DictWrapper({'feat_name': 'I6', 'feat_num': 100, 'embed_dim': 8}), DictWrapper({'feat_name': 'I7', 'feat_num': 100, 'embed_dim': 8}), DictWrapper({'feat_name': 'I8', 'feat_num': 100, 'embed_dim': 8}), DictWrapper({'feat_name': 'I9', 'feat_num': 100, 'embed_dim': 8}), DictWrapper({'feat_name': 'I10', 'feat_num': 100, 'embed_dim': 8}), DictWrapper({'feat_name': 'I11', 'feat_num': 100, 'embed_dim': 8}), DictWrapper({'feat_name': 'I12', 'feat_num': 100, 'embed_dim': 8}), DictWrapper({'feat_name': 'I13', 'feat_num': 100, 'embed_dim': 8})])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fm.sparse_feature_columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
